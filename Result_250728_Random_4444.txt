==================================================
### Prüfungsprotokoll ###
Datum: 28.07.2025, 09:41:42 Uhr
Modus: Practice
Auswahl: Random (50 Fragen)
Random Seed: 4444

--- Endergebnis ---
Punkte: 41 von 50
Erfolgsquote: 82.00%
==================================================

--- Frage Nr. 319 (Richtig) ---
Frage: How can a user get the MOST detailed information about individual table storage details in Snowflake?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
D - table_storage_metrics view
it's correct. https://docs.snowflake.com/en/sql-reference/account-usage/table_storage_metrics
--------------------------------------------------

--- Frage Nr. 978 (Richtig) ---
Frage: What privilege is needed for a Snowflake user to see the definition of a secure view?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
The definition of a secure view is only exposed to authorized users (i.e. users who have been granted the role that owns the view).  However, users that have been granted IMPORTED PRIVILEGES privilege on the SNOWFLAKE database or another shared database have access to secure view definitions via the VIEWS Account Usage view.  Users granted the ACCOUNTADMIN role or the SNOWFLAKE.OBJECT_VIEWER database role can also see secure view definitions via this view. The preferred, least-privileged means of access is the SNOWFLAKE.OBJECT_VIEWER database role.  https://docs.snowflake.com/en/user-guide/views-secure
--------------------------------------------------

--- Frage Nr. 731 (Richtig) ---
Frage: What is used to diagnose and troubleshoot network connections to Snowflake?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
SnowCD (i.e. Snowflake Connectivity Diagnostic Tool) helps users to diagnose and troubleshoot their network connection to Snowflake.
--------------------------------------------------

--- Frage Nr. 592 (Richtig) ---
Frage: Which Snowflake function and command combination should be used to convert rows in a relational table to a single VARIANT column, and unload the rows into a file in JSON format? (Choose two.)
Deine Antwort:     C, E
Korrekte Antwort:  C, E

Erklärung:
CE - seen this question multiple times
--------------------------------------------------

--- Frage Nr. 244 (Richtig) ---
Frage: A user has an application that writes a new file to a cloud storage location every 5 minutes.

What would be the MOST efficient way to get the files into Snowflake?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
D. Set up cloud provider notifications on the file location and use Snowpipe with auto-ingest.
--------------------------------------------------

--- Frage Nr. 754 (Richtig) ---
Frage: A Snowflake user has two tables that contain numeric values and is trying to find out which values are present in both tables.

Which set operator should be used?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
https://docs.snowflake.com/en/sql-reference/operators-query INTERSECT Returns rows from one query’s result set which also appear in another query’s result set, with duplicate elimination.
Correct
--------------------------------------------------

--- Frage Nr. 377 (Richtig) ---
Frage: Which command should be used to look into the validity of an XML object in Snowflake?
Deine Antwort:     C
Korrekte Antwort:  C

Erklärung:
C. PARSE_XML: This function parses a string into an XML data type. While it will raise an error if the XML is malformed, its primary purpose is parsing, not just checking validity. You'd typically use it when you intend to consume the XML. 

D. CHECK_XML: This function is specifically designed to validate if a VARCHAR string contains a well-formed XML document. It returns TRUE if the XML is well-formed and FALSE otherwise, without attempting to parse it into an XML data type. This directly answers the need to "look into the validity."

The final answer is D
--------------------------------------------------

--- Frage Nr. 1060 (Richtig) ---
Frage: Where would a Snowflake user find information about query activity from 90 days ago?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html
https://docs.snowflake.com/en/sql-reference/account-usage/query_history
--------------------------------------------------

--- Frage Nr. 976 (Richtig) ---
Frage: What step can reduce data spilling in Snowflake?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
Answer is A: https://docs.snowflake.com/en/user-guide/ui-query-profile For some operations (e.g. duplicate elimination for a huge data set), the amount of memory available for the compute resources used to execute the operation might not be sufficient to hold intermediate results. As a result, the query processing engine will start spilling the data to local disk. If the local disk space is not sufficient, the spilled data is then saved to remote disks.  This spilling can have a profound effect on query performance (especially if remote disk is used for spilling). To alleviate this, we recommend:  Using a larger warehouse (effectively increasing the available memory/local disk space for the operation), and/or  Processing data in smaller batches.
Answer A is correct! Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs out of memory, which results in bytes “spilling” onto storage. https://docs.snowflake.com/en/user-guide/performance-query-warehouse
A correct - https://community.snowflake.com/s/article/Performance-impact-from-local-and-remote-disk-spilling
--------------------------------------------------

--- Frage Nr. 406 (Richtig) ---
Frage: What are potential impacts of storing non-native values like dates and timestamps in a VARIANT column in Snowflake?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
Non-native values such as dates and timestamps are stored as strings when loaded into a VARIANT column, so operations on these values could be slower and also consume more space than when stored in a relational column with the corresponding data type.
https://docs.snowflake.com/en/sql-reference/data-types-semistructured
--------------------------------------------------

--- Frage Nr. 664 (Falsch) ---
Frage: What command would a user execute to load unstructured data files into a Snowflake internal stage?
Deine Antwort:     D
Korrekte Antwort:  A

Erklärung:
Copy into command will load data from stage to a table. Data needs to be in stage already. In this case, I think I woul go with A. PUT.   COPY INTO <table> Loads data from staged files to an existing table. The files must already be staged in one of the following locations: Named internal stage, Named external stage. To load data from our local machine into the Snowflake Internal stage: 2. Using PUT command, copy the files from the local folder into snowflake internal stage created in earlier step.
--------------------------------------------------

--- Frage Nr. 536 (Richtig) ---
Frage: Which type of Snowflake virtual warehouse provides 16 times the memory for each node, and is recommended for larger workloads like Machine Learning (ML) training?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
Agree with D
SnowPark-optimized virtual warehouse is designed for high memory requirements, such as ML workloads, ML training datasets, and other memory-intensive tasks. It provides 16 times more memory per node than a standard virtual warehouse, making it an excellent choice for handling such workloads on a single virtual warehouse.
--------------------------------------------------

--- Frage Nr. 157 (Richtig) ---
Frage: What happens when a cloned table is replicated to a secondary database? (Choose two.)
Deine Antwort:     C, D
Korrekte Antwort:  C, D

Erklärung:
Replication and Cloning Cloned objects are replicated physically rather than logically to secondary databases. That is, cloned tables in a standard database do not contribute to the overall data storage unless or until DML operations on the clone add to or modify existing data. However, when a cloned table is replicated to a secondary database, the physical data is also replicated, increasing the data storage usage for your account.  https://docs.snowflake.com/en/user-guide/account-replication-considerations#replication-and-cloni
--------------------------------------------------

--- Frage Nr. 605 (Richtig) ---
Frage: What is the MINIMUM Snowflake edition required to use the column-level security feature?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
https://docs.snowflake.com/en/user-guide/intro-editions
This feature requires Enterprise Edition (or higher). To inquire about upgrading, please contact Snowflake Support. https://docs.snowflake.com/en/user-guide/security-column-intro
C. Business Critical  Explanation: Column-level security in Snowflake, such as column masking policies, is available starting from the Business Critical edition, providing more granular control over data access.  Why the others are wrong:   A. Standard: The Standard edition does not support column-level security features.  B. Enterprise: Although the Enterprise edition includes many features, column-level security is not available in this edition.  D. Virtual Private Snowflake (VPS): While VPS includes enhanced security features, it is not the minimum edition for column-level security.
--------------------------------------------------

--- Frage Nr. 329 (Richtig) ---
Frage: What happens when a network policy includes values that appear in both the allowed and blocked IP address lists?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
It's B - https://docs.snowflake.com/en/user-guide/network-policies "When a network policy includes values in both the allowed and blocked IP address lists, Snowflake applies the blocked IP address list first."
B. When a network policy includes values in both the allowed and blocked IP address lists, Snowflake applies the blocked IP address list first. https://docs.snowflake.com/en/user-guide/network-policies
A. Those IP addresses are allowed access to the Snowflake account as Snowflake applies the allowed IP address list first.  Explanation:  In Snowflake's network policy configuration, the allowed IP address list takes precedence over the blocked IP address list. If an IP address is listed in both the allowed and blocked lists, Snowflake applies the rule from the allowed list, allowing access. The allowed list is processed before the blocked list, and if a match is found in the allowed list, the access is granted.  So, the correct answer is A. Those IP addresses are allowed access to the Snowflake account as Snowflake applies the allowed IP address list first.
--------------------------------------------------

--- Frage Nr. 696 (Richtig) ---
Frage: What role is required to use Partner Connect?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
Partner Connect is limited to account administrators (i.e. users with the ACCOUNTADMIN role) who have a verified email address in Snowflake: To use Partner Connect, you must switch to the ACCOUNTADMIN role or contact someone in your organization who has the role.
https://docs.snowflake.com/en/user-guide/ecosystem-partner-connect
--------------------------------------------------

--- Frage Nr. 290 (Richtig) ---
Frage: What is the maximum length of time travel available in the Snowflake Standard Edition?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
Only 1 day
--------------------------------------------------

--- Frage Nr. 763 (Richtig) ---
Frage: Which encryption type will enable client-side encryption for a directory table?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
SNowflake_full for client and snowflake_sse for server side
https://suya-huang.medium.com/snowflakes-client-side-encryption-and-server-side-encryption-4c7a83427010
--------------------------------------------------

--- Frage Nr. 854 (Richtig) ---
Frage: Which object-level parameters can be set to help control query processing and concurrency? (Choose two).
Deine Antwort:     D, E
Korrekte Antwort:  D, E

Erklärung:
D correct - Snowflake provides some object-level parameters that can be set to help control query processing and concurrency:   STATEMENT_QUEUED_TIMEOUT_IN_SECONDS   STATEMENT_TIMEOUT_IN_SECONDS  https://docs.snowflake.com/en/user-guide/warehouses-overview
Statements A and E are correct: https://docs.snowflake.com/en/sql-reference/parameters#statement-queued-timeout-in-seconds STATEMENT_QUEUED_TIMEOUT_IN_SECONDS Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged.
--------------------------------------------------

--- Frage Nr. 1151 (Richtig) ---
Frage: Which command should a Snowflake user execute to load data into a table?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
B correct
correct
--------------------------------------------------

--- Frage Nr. 361 (Richtig) ---
Frage: Which Snowflake data type is used to store JSON key value pairs?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
D is correct https://docs.snowflake.com/en/sql-reference/data-types-semistructured VARIANT and OBJECT can contain key-value pairs
--------------------------------------------------

--- Frage Nr. 153 (Richtig) ---
Frage: Which command can be used to load data files into a Snowflake stage?
Deine Antwort:     C
Korrekte Antwort:  C

Erklärung:
(up)load to stage - PUT (up)load from stage - COPY INTO FROM @ unload to stage - COPY INTO @ download from stage - GET
https://docs.snowflake.com/en/user-guide/data-load-local-file-system-stage.html
--------------------------------------------------

--- Frage Nr. 120 (Richtig) ---
Frage: The Information Schema and Account Usage Share provide storage information for which of the following objects? (Choose three.)
Deine Antwort:     B, C, D
Korrekte Antwort:  B, C, D

Erklärung:
select * from snowflake.information_schema.views where table_name like '%STORAGE%'  and table_schema in ('ACCOUNT_USAGE','INFORMATION_SCHEMA');  TABLE_CATALOG TABLE_SCHEMA TABLE_NAME SNOWFLAKE ACCOUNT_USAGE DATABASE_STORAGE_USAGE_HISTORY SNOWFLAKE ACCOUNT_USAGE STAGE_STORAGE_USAGE_HISTORY SNOWFLAKE ACCOUNT_USAGE STORAGE_USAGE SNOWFLAKE ACCOUNT_USAGE TABLE_STORAGE_METRICS SNOWFLAKE INFORMATION_SCHEMA TABLE_STORAGE_METRICS
We can get User info too. The Account Usage view can be used to query a list of all users in the account. The data is retained for 365 days (1 year).  https://docs.snowflake.com/en/sql-reference/account-usage/users.html
--------------------------------------------------

--- Frage Nr. 436 (Richtig) ---
Frage: When creating a virtual warehouse, what setting should be used to avoid both over-provisioning resources and auto-scaling when there is increased concurrency?
Deine Antwort:     C
Korrekte Antwort:  C

Erklärung:
To avoid both over-provisioning resources and auto-scaling when there is increased concurrency in a Snowflake virtual warehouse, you should use the MAX_CLUSTER_COUNT = 1 setting.

MAX_CLUSTER_COUNT: This parameter defines the maximum number of independent compute clusters that a multi-cluster warehouse can scale out to. If MAX_CLUSTER_COUNT is set to 1, the warehouse will operate as a single-cluster warehouse. This means it will never auto-scale by adding more clusters, even if there's increased concurrency and queries are queuing. This directly prevents auto-scaling due to concurrency.

By limiting to a single cluster, you then have full control over that single cluster's WAREHOUSE_SIZE to avoid over-provisioning. If you choose an appropriate size (e.g., MEDIUM, SMALL, etc.) for your typical workload with MAX_CLUSTER_COUNT = 1, you prevent both unnecessary scaling and excessive base provisioning.
--------------------------------------------------

--- Frage Nr. 595 (Richtig) ---
Frage: When sharing data in Snowflake, what privileges does a Provider need to grant along with a share? (Choose two.)
Deine Antwort:     C, D
Korrekte Antwort:  C, D

Erklärung:
https://docs.snowflake.com/en/user-guide/data-sharing-gs#grant-database-roles-to-a-share
GRANT USAGE ON DATABASE sales_db TO SHARE sales_s; GRANT USAGE ON SCHEMA sales_db.aggregates_eula TO SHARE sales_s;  GRANT SELECT ON TABLE sales_db.aggregates_eula.aggregate_1 TO SHARE sales_s;
--------------------------------------------------

--- Frage Nr. 1129 (Richtig) ---
Frage: How are micro-partitions typically generated in Snowflake?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
Micro-partitions are generated automatically, you can do it manually though by adding CLUSTER BY statement
A is correct
--------------------------------------------------

--- Frage Nr. 918 (Richtig) ---
Frage: By default, how long is the standard retention period for Time Travel across all Snowflake accounts?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
B is correct. The standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts: For Snowflake Standard Edition, the retention period can be set to 0 (or unset back to the default of 1 day) at the account and object level (i.e. databases, schemas, and tables).
B correct - https://docs.snowflake.com/en/user-guide/data-time-travel
--------------------------------------------------

--- Frage Nr. 468 (Richtig) ---
Frage: Which Query Profile operator provides information on pruning efficiency?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
A. TableScan: This operator represents the action of reading data from a Snowflake table. Snowflake's architecture, based on micro-partitions and automatic clustering, allows for significant pruning. The Query Profile for a TableScan operator will show metrics such as:

Partitions scanned: The actual number of micro-partitions that were read. Partitions total: The total number of micro-partitions that comprise the table. The ratio of "Partitions scanned" to "Partitions total" directly indicates the pruning efficiency. A low ratio means high pruning efficiency.
--------------------------------------------------

--- Frage Nr. 749 (Richtig) ---
Frage: Which feature is integrated to support Multi-Factor Authentication (MFA) at Snowflake?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
correct
--------------------------------------------------

--- Frage Nr. 1058 (Richtig) ---
Frage: A company strongly encourages all Snowflake users to self-enroll in Snowflake's default Multi-Factor Authentication (MFA) service to provide increased login security for users connecting to Snowflake.

Which application will the Snowflake users need to install on their devices in order to connect with MFA?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
https://docs.snowflake.com/en/user-guide/ui-preferences.html#:~:text=Enrolling%20in%20MFA%20(Multi%2DFactor%20Authentication),-MFA%20is%20a&text=This%20second%20form%20of%20authentication,smart%20phone%20or%20similar%20device.
https://docs.snowflake.com/en/user-guide/security-mfa.html
--------------------------------------------------

--- Frage Nr. 457 (Richtig) ---
Frage: How should a Snowflake user access a third-party SaaS service to process unstructured data?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
To integrate a third‑party SaaS service for processing unstructured data in Snowflake—such as using Amazon Textract, Google Document AI, or Azure Computer Vision—the user should leverage External Functions. External Functions are Snowflake UDFs that call code executed outside of the Snowflake engine via a secure proxy (e.g., Amazon API Gateway), enabling seamless invocation of external HTTP‑based services without unloading data or moving it out of Snowflake’s environment.
--------------------------------------------------

--- Frage Nr. 636 (Falsch) ---
Frage: What happens to foreign key constraints when a table is cloned to another database?
Deine Antwort:     D
Korrekte Antwort:  B

Erklärung:
C. The cloned table will lose all references to the primary key. True. This is the most accurate statement. When an individual table is cloned, any foreign key constraints it had (which are "references to primary keys" in other tables) are not transferred to the cloned table. The cloned table will have no foreign key constraints unless you explicitly add them.
--------------------------------------------------

--- Frage Nr. 268 (Falsch) ---
Frage: According to Snowflake best practice recommendations, which role should be used to create databases?
Deine Antwort:     C
Korrekte Antwort:  B

Erklärung:
The system administrator (SYSADMIN) role includes the privileges to create warehouses, databases, and all database objects (schemas, tables, etc.).  https://docs.snowflake.com/en/user-guide/security-access-control-considerations.html#:~:text=The%20system%20administrator%20(SYSADMIN)%20role,%2C%20tables%2C%20etc.).
--------------------------------------------------

--- Frage Nr. 1018 (Richtig) ---
Frage: A user is preparing to load data from an external stage.

Which practice will provide the MOST efficient loading performance?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
Both internal (i.e. Snowflake) and external (Amazon S3, Google Cloud Storage, or Microsoft Azure) stage references can include a path (or prefix in AWS terminology). When staging regular data sets, we recommend partitioning the data into logical paths that include identifying details such as geographical location or other source identifiers, along with the date when the data was written.  Organizing your data files by path lets you copy any fraction of the partitioned data into Snowflake with a single command. This allows you to execute concurrent COPY statements that match a subset of files, taking advantage of parallel operations.  https://docs.snowflake.com/en/user-guide/data-load-considerations-stage.html#organizing-data-by-path
https://docs.snowflake.com/en/user-guide/data-load-considerations-stage.html#organizing-data-by-path
--------------------------------------------------

--- Frage Nr. 962 (Richtig) ---
Frage: How can a Snowflake user post-process the result of SHOW FILE FORMATS?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
To post-process the output of File Format command, you can use the RESULT_SCAN function, which treats the output as a table that can be queried.
A correct. first run SHOW FILE FORMATS then SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID(-1)))  https://docs.snowflake.com/en/sql-reference/functions/result_scan#usage-notes
A correct - https://docs.snowflake.com/en/sql-reference/sql/show-file-formats
--------------------------------------------------

--- Frage Nr. 1157 (Richtig) ---
Frage: Which use case will always cause an exploding join in Snowflake?
Deine Antwort:     C
Korrekte Antwort:  C

Erklärung:
“Exploding” joins  One of the common mistakes SQL users make is joining tables without providing a join condition (resulting in a “Cartesian product”), or providing a condition where records from one table match multiple records from another table. For such queries, the Join operator produces significantly (often by orders of magnitude) more tuples than it consumes.
C. A query that has not specified join criteria for tables.
C https://select.dev/posts/snowflake-query-profile
--------------------------------------------------

--- Frage Nr. 1086 (Richtig) ---
Frage: Which file formats are supported for unloading data from Snowflake? (Choose two.)
Deine Antwort:     B, E
Korrekte Antwort:  B, E

Erklärung:
BE are correct
Delimited files (CSV, TSV, etc.) Any valid delimiter is supported; default is comma (i.e. CSV). JSON Parquet https://docs.snowflake.com/en/user-guide/intro-summary-unloading#output-data-file-details
https://docs.snowflake.com/en/user-guide/intro-summary-unloading.html#output-data-file-details
CSV, JSON, Parquet
--------------------------------------------------

--- Frage Nr. 689 (Richtig) ---
Frage: Which Snowflake feature allows administrators to identify unused data that may be archived or deleted?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
Access history in Snowflake provides the following benefits pertaining to read and write operations: Data discovery: Discover unused data to determine whether to archive or delete the data.  https://docs.snowflake.com/en/user-guide/access-history
https://docs.snowflake.com/en/user-guide/access-history#:~:text=Discover%20unused%20data%20to%20determine%20whether%20to%20archive%20or%20delete%20the%20data.
Each row in the ACCESS_HISTORY view contains a single record per SQL statement. The record describes the columns the query accessed directly and indirectly (i.e. the underlying tables that the data for the query comes from). These records facilitate regulatory compliance auditing and provide insights on popular and frequently accessed tables and columns since there is a direct link between the user (i.e. query operator), the query, the table or view, the column, and the data
Data classification is to classify personal data. it is a governance feature, https://docs.snowflake.com/en/user-guide/governance-classify. So answer should be A.
--------------------------------------------------

--- Frage Nr. 644 (Falsch) ---
Frage: What identifiers are supported when creating a Snowflake account hostname? (Choose two.)
Deine Antwort:     A, C
Korrekte Antwort:  A, D

Erklärung:
https://docs.snowflake.com/en/user-guide/admin-account-identifier As mentioned by 37ceea2, there are 2 options: 1. account name in your organization (preferred) 2. Account locator in region. The question is a bit confusing, but since there are 2 answers expected, Cloud region and Account locator seems to be the right option.
https://docs.snowflake.com/en/user-guide/admin-account-identifier Finding the region and locator for an account If you can connect to your Snowflake account, you can query the following context functions to identify the region and account locator for the Snowflake account you are connected to:  CURRENT_REGION retrieves the region in which your account is located.  CURRENT_ACCOUNT retrieves the account locator.  If you are unable to connect to Snowflake, contact the Snowflake administrator for your account to retrieve this information. This is the most close to the question's topic, I was able to find/
--------------------------------------------------

--- Frage Nr. 280 (Richtig) ---
Frage: Which cache type is used to cache data output from SQL queries?
Deine Antwort:     B
Korrekte Antwort:  B

Erklärung:
Result Cache:  Which holds the results of every query executed in the past 24 hours. These are available across virtual warehouses, so query results returned to one user is available to any other user on the system who executes the same query, provided the underlying data has not changed. Local Disk Cache:  Which is used to cache data used by SQL queries. Whenever data is needed for a given query it's retrieved from the Remote Disk storage, and cached in SSD and memory.
--------------------------------------------------

--- Frage Nr. 3 (Richtig) ---
Frage: True or False: A single database can exist in more than one Snowflake account.
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
The question is where the single database (instance) can exist across multiple accounts. Replication creates a separate instance and keeps it in sync with primary DB, but the DB instance can only belong to one account. Its a tricky question
Through database replication, should be A https://docs.snowflake.com/en/user-guide/database-replication-intro.html
--------------------------------------------------

--- Frage Nr. 589 (Richtig) ---
Frage: Which security models are used in Snowflake to manage access control? (Choose two.)
Deine Antwort:     A, D
Korrekte Antwort:  A, D

Erklärung:
https://docs.snowflake.com/en/user-guide/security-access-control-overview  Snowflake’s approach to access control combines aspects from the following models:  Discretionary Access Control (DAC): Each object has an owner, who can in turn grant access to that object.  Role-based Access Control (RBAC): Access privileges are assigned to roles, which are in turn assigned to users.  User-based Access Control (UBAC): Access privileges are assigned directly to users. Access control considers privileges assigned directly to users only when USE SECONDARY ROLE is set to ALL.
It's the same response from Gemini, GPT and Claude ;-)  Be careful, because they often give different answers. They fail a lot of these tests
--------------------------------------------------

--- Frage Nr. 230 (Falsch) ---
Frage: Which SQL commands, when committed, will consume a stream and advance the stream offset? (Choose two.)
Deine Antwort:     C, D
Korrekte Antwort:  A, C

Erklärung:
AC The stream position (i.e. offset) is advanced when the stream is used in a DML statement. The position is updated at the end of the transaction to the beginning timestamp of the transaction. The stream describes change records starting from the current position of the stream and ending at the current transactional timestamp.  To ensure multiple statements access the same change records in the stream, surround them with an explicit transaction statement (BEGIN .. COMMIT). An explicit transaction locks the stream, so that DML updates to the source object are not reported to the stream until the transaction is committed.
https://docs.snowflake.com/en/user-guide/streams-intro  To advance the offset of a stream to the current table version without consuming the change data in a DML operation, complete either of the following actions:  Recreate the stream (using the CREATE OR REPLACE STREAM syntax).  Insert the current change data into a temporary table. In the INSERT statement, query the stream but include a WHERE clause that filters out all of the change data (e.g. WHERE 0 = 1).
--------------------------------------------------

--- Frage Nr. 191 (Falsch) ---
Frage: What actions will prevent leveraging of the ResultSet cache? (Choose two.)
Deine Antwort:     B, E
Korrekte Antwort:  A, E

Erklärung:
A. Removing a column from the query SELECT list:
The Result Set Cache relies on the exact query text (or a functionally identical query plan). If even a single column is removed or added to the SELECT list, the query text changes, and thus it will not be considered the same query as the cached one. This will prevent leveraging the cache.
Correct.

B. Stopping the virtual warehouse that the query is running against:
    The Result Set Cache is a global cache maintained by the Cloud Services layer, separate from individual virtual warehouses. Stopping, starting, or resizing a virtual warehouse does not affect the validity of cached query results. If an identical query is run on a different or restarted warehouse, it can still hit the cache.
Incorrect.

C. Clustering of the data used by the query:
Clustering is a storage optimization. If the data itself is modified in the micro-partitions that were used by the cached query (e.g., due to DML operations like INSERT, UPDATE, DELETE, or a re-clustering process that changes the underlying data files), then the Result Set Cache for queries relying on that data will be invalidated. While the act of clustering itself is a data reorganization, the direct cause of cache invalidation is the change in the underlying data. If "Clustering" implies that the data relevant to the query has been modified/reorganized, then the cache would be prevented. However, just having clustering defined doesn't prevent it; it's the change to the data. Compared to A and E, this is less direct as a preventative action.
Less direct, but can lead to invalidation if data changes.

D. Executing the RESULTS_SCAN() table function:
The RESULTS_SCAN() table function is specifically designed to retrieve the results of a previous query from the Result Set Cache (or a temporary internal stage if the result is too large for the cache). Therefore, executing this function actively leverages the cache; it does not prevent it.
Incorrect.

E. Changing a column that is not in the cached query:
The Result Set Cache is invalidated if any data within any column of any table involved in the cached query is modified. Snowflake's cache invalidation is based on the underlying micro-partitions. If a micro-partition involved in the original query is changed (even a column not directly selected or filtered), the checksum for that micro-partition changes, and any cached query results dependent on it are invalidated.
Correct.

Therefore, the two actions that will prevent leveraging the Result Set Cache are changes to the query text or changes to the underlying data.

The final answer is  
A,E
--------------------------------------------------

--- Frage Nr. 1142 (Falsch) ---
Frage: Which parameter prevents streams on tables from becoming stale?
Deine Antwort:     B
Korrekte Antwort:  A

Erklärung:
A. MAX_DATA_EXTENSION_TIME_IN_DAYS: This is not a standard Snowflake parameter related to stream staleness or data retention. The closest real parameter would be MAX_DATA_RETENTION_TIME_IN_DAYS, but "EXTENSION" is incorrect.

B. MIN_DATA_RETENTION_TIME_IN_DAYS: This is a parameter that can be set at the table, schema, or database level. It specifies the minimum number of days for which historical data (for Time Travel and Fail-safe) is retained. If a stream is active on a table, and its data retention period (effective from DATA_RETENTION_TIME_IN_DAYS) would drop below the stream's required history, setting MIN_DATA_RETENTION_TIME_IN_DAYS can prevent the historical data from being purged too early, thus preventing the stream from becoming stale. This parameter directly addresses the problem of streams losing their historical context.

C. LOCK_TIMEOUT: This is a parameter related to transaction locking and concurrency control. It specifies how long a statement waits to acquire a lock. It has no bearing on stream staleness.

D. STALE_AFTER: This is not a standard Snowflake parameter. While streams can become stale, there isn't a parameter with this name to prevent it.

Therefore, MIN_DATA_RETENTION_TIME_IN_DAYS is the parameter designed to ensure sufficient data retention for streams to function correctly and avoid becoming stale.

The final answer is  
B
​
--------------------------------------------------

--- Frage Nr. 841 (Falsch) ---
Frage: What is a characteristic of the maintenance of a materialized view?
Deine Antwort:     D
Korrekte Antwort:  C

Erklärung:
answer is C
C: Materialized views are automatically and transparently maintained by Snowflake. A background service updates the materialized view after changes are made to the base table. This is more efficient and less error-prone than manually maintaining the equivalent of a materialized view at the application level. Materialized views are designed to improve query performance for workloads composed of common, repeated query patterns. However, materializing intermediate results incurs additional costs. As such, before creating any materialized views, you should consider whether the costs are offset by the savings from re-using these results frequently enough.
--------------------------------------------------

--- Frage Nr. 661 (Richtig) ---
Frage: For the ALLOWED_VALUES tag property, what is the MAXIMUM number of possible string values for a single tag?
Deine Antwort:     D
Korrekte Antwort:  D

Erklärung:
it's 300
None of them As of Feb 2024, it's 300  The ALLOWED_VALUES tag property enables specifying the possible string values that can be assigned to the tag when the tag is set on an object. The maximum number of possible string values for a single tag is 300.  https://docs.snowflake.com/en/user-guide/object-tagging#label-object-tagging-specify-tag-values
https://docs.snowflake.com/en/user-guide/object-tagging#label-object-tagging-specify-tag-values:~:text=The%20ALLOWED_VALUES%20tag%20property%20enables%20specifying%20the%20possible%20string%20values%20that%20can%20be%20assigned%20to%20the%20tag%20when%20the%20tag%20is%20set%20on%20an%20object.%20The%20maximum%20number%20of%20possible%20string%20values%20for%20a%20single%20tag%20is%2050.
--------------------------------------------------

--- Frage Nr. 1133 (Richtig) ---
Frage: Which command should be used when loading many flat files into a single table?
Deine Antwort:     C
Korrekte Antwort:  C

Erklärung:
Step 1. Upload (i.e. stage) one or more data files to a Snowflake stage (named internal stage or table/user stage) using the PUT command. Step 2. Use the COPY INTO <table> command to load the contents of the staged file(s) into a Snowflake database table.
Correct answer
--------------------------------------------------

--- Frage Nr. 1126 (Richtig) ---
Frage: How often are encryption keys automatically rotated by Snowflake?
Deine Antwort:     A
Korrekte Antwort:  A

Erklärung:
All Snowflake-managed keys are automatically rotated by Snowflake when they are more than 30 days old
https://docs.snowflake.com/en/user-guide/security-encryption-manage.html#:~:text=it%20is%20usable.-,Encryption%20Key%20Rotation,and%20new%20keys%20are%20created.
--------------------------------------------------

--- Frage Nr. 928 (Falsch) ---
Frage: Which command will unload data from a table into an external stage?
Deine Antwort:     D
Korrekte Antwort:  C

Erklärung:
C correct
--------------------------------------------------

